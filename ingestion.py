'''
Purpose: PDF ë¡œë”© â†’ í…ìŠ¤íŠ¸ ì •ì œ â†’ Pillar/BP ID ê°ì§€ â†’ Chunking â†’ ìœ íš¨ì„± ê²€ì¦
'''
import os
import re
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

from langchain_aws import BedrockEmbeddings
from langchain_community.vectorstores import FAISS

DOCUMENT_DIR = "./docs"
VECTOR_DB_DIR = "./vectorstore"

# Pillar ì •ì˜ (ë¬¸ì„œ ìˆœì„œëŒ€ë¡œ)
PILLAR_PATTERNS = {
    "Operational Excellence": ["operational excellence", "OPS0", "OPS1"],
    "Security": ["security pillar", "SEC0", "SEC1", "SEC2"],
    "Reliability": ["reliability pillar", "REL0", "REL1"],
    "Performance Efficiency": ["performance efficiency", "PERF0", "PERF1"],
    "Cost Optimization": ["cost optimization", "COST0", "COST1"],
    "Sustainability": ["sustainability pillar", "SUS0", "SUS1"]
}

def detect_pillar(text: str, current_pillar: str = "General") -> str:
    """í…ìŠ¤íŠ¸ì—ì„œ Pillar ê°ì§€ (ë” ì •êµí•œ ë²„ì „)"""
    text_lower = text.lower()
    
    for pillar, keywords in PILLAR_PATTERNS.items():
        for keyword in keywords:
            if keyword.lower() in text_lower:
                return pillar
    
    return current_pillar  # ê°ì§€ ì•ˆë˜ë©´ ì´ì „ Pillar ìœ ì§€

def detect_best_practice_id(text: str) -> str:
    """Best Practice ID ê°ì§€ (íŒ¨í„´ í™•ì¥)"""
    # SEC01-BP01, OPS02-BP03, REL01-BP02 ë“±
    patterns = [
        r'([A-Z]{2,4}\d{1,2}-BP\d{1,2})',  # SEC01-BP01
        r'([A-Z]{2,4}\d{1,2}\s*-\s*BP\s*\d{1,2})',  # SEC01 - BP 01 (ê³µë°± í¬í•¨)
        r'(Best practice \d+\.\d+)',  # Best practice 1.1
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            return match.group(1).replace(" ", "").upper()
    
    return ""

def clean_text(text: str) -> str:
    """ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì •ë¦¬"""
    # ë°˜ë³µ íŒ¨í„´ ì œê±°
    text = re.sub(r'(fault tolerance, high availability,?\s*)+', '', text)
    text = re.sub(r'(reliability pillar, cloud architecture best practices,?\s*)+', '', text)
    text = re.sub(r'(AWS Well-Architected Framework,?\s*)+', '', text, flags=re.IGNORECASE)
    # í—¤ë”/í‘¸í„° íŒ¨í„´ ì œê±°
    text = re.sub(r'^\s*\d+\s*$', '', text, flags=re.MULTILINE)  # í˜ì´ì§€ ë²ˆí˜¸ë§Œ ìˆëŠ” ì¤„
    # ê³¼ë„í•œ ê³µë°± ì •ë¦¬
    text = re.sub(r'\n{3,}', '\n\n', text)
    text = re.sub(r' {2,}', ' ', text)
    return text.strip()

def is_valid_chunk(text: str) -> bool:
    """ìœ íš¨í•œ ì²­í¬ì¸ì§€ ê²€ì¦"""
    if len(text) < 100:
        return False
    if text.count("well-architected") > 10:
        return False
    if text.count("...") > 10:  # ëª©ì°¨
        return False
    if text.count("â€¢") > 20 and len(text) < 500:  # ëª©ë¡ë§Œ ìˆëŠ” ê²½ìš°
        return False
    return True

def load_and_clean_documents(path: str):
    """PDF ë¡œë”© + ì •ì œ + Pillar ëˆ„ì  ì¶”ì """
    documents = []
    current_pillar = "General"
    
    for filename in os.listdir(path):
        if filename.endswith(".pdf"):
            filepath = os.path.join(path, filename)
            print(f"ğŸ“„ Loading: {filename}")
            
            loader = PyPDFLoader(filepath)
            raw_docs = loader.load()
            
            # í˜ì´ì§€ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬ (Pillar ëˆ„ì  ì¶”ì )
            for doc in sorted(raw_docs, key=lambda x: x.metadata.get("page", 0)):
                cleaned_text = clean_text(doc.page_content)
                
                # Pillar ê°ì§€ (í˜„ì¬ í˜ì´ì§€ì—ì„œ ìƒˆ Pillar ë°œê²¬í•˜ë©´ ì—…ë°ì´íŠ¸)
                current_pillar = detect_pillar(cleaned_text, current_pillar)
                bp_id = detect_best_practice_id(cleaned_text)
                
                if is_valid_chunk(cleaned_text):
                    enhanced_metadata = {
                        "source": filename,
                        "page": doc.metadata.get("page", 0) + 1,  # 1-indexed
                        "pillar": current_pillar,
                        "best_practice_id": bp_id,
                    }
                    
                    documents.append(Document(
                        page_content=cleaned_text,
                        metadata=enhanced_metadata
                    ))
    
    print(f"âœ… ì •ì œ í›„ ë¬¸ì„œ ìˆ˜: {len(documents)}")
    return documents

def split_documents(documents):
    """ì˜ë¯¸ ë‹¨ìœ„ Chunking -> Semantic Chunking"""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,
        chunk_overlap=300,
        separators=[
            "\n\n\n",
            "\n\n",
            "\nBest practice",
            "\nRequired:",
            "\nRecommended:",
            "\nâ€¢ ",
            "\n",
            ". ",
            " "
        ]
    )
    
    chunks = text_splitter.split_documents(documents)
    
    # ì²­í¬ë³„ BP ID ì¬ê°ì§€
    valid_chunks = []
    for chunk in chunks:
        if is_valid_chunk(chunk.page_content):
            # ì²­í¬ ë‚´ì—ì„œ BP ID ë‹¤ì‹œ ì°¾ê¸°
            bp_id = detect_best_practice_id(chunk.page_content)
            if bp_id:
                chunk.metadata["best_practice_id"] = bp_id
            valid_chunks.append(chunk)
    
    print(f"âœ… ìœ íš¨ ì²­í¬ ìˆ˜: {len(valid_chunks)}")
    return valid_chunks


def create_vectorstore(chunks):
    """Embedding ìƒì„± + FAISS ì €ì¥"""
    
    print("\n" + "="*50)
    print("ğŸ”„ Embedding ìƒì„± ì‹œì‘")
    print("="*50)
    
    # Bedrock Titan Embeddings
    embeddings = BedrockEmbeddings(
        model_id="amazon.titan-embed-text-v2:0",
        region_name="us-east-1"
    )
    
    print(f"ğŸ“Š ì´ {len(chunks)}ê°œ ì²­í¬ ì„ë² ë”© ì¤‘...")
    print("â³ ì˜ˆìƒ ì†Œìš” ì‹œê°„: 5-10ë¶„")
    
    # FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    vectorstore = FAISS.from_documents(
        documents=chunks,
        embedding=embeddings
    )
    
    # ë¡œì»¬ ì €ì¥
    os.makedirs(VECTOR_DB_DIR, exist_ok=True)
    vectorstore.save_local(VECTOR_DB_DIR)
    
    print(f"âœ… VectorStore ë¡œì»¬ ì €ì¥ ì™„ë£Œ: {VECTOR_DB_DIR}")
    print(f"   - index.faiss")
    print(f"   - index.pkl")
    
    return vectorstore


def test_retrieval(vectorstore):
    """ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    test_queries = [
        "How should IAM authentication be designed?",
        "What is the tagging strategy for cost optimization?",
        "How should Recovery Time Objective (RTO) and Recovery Point Objective (RPO) be set for disaster recovery?",
        "What are the methods for optimizing Lambda function performance?"
    ]
    
    print("\n" + "="*50)
    print("ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    print("="*50)
    
    for query in test_queries:
        print(f"\nì§ˆë¬¸: {query}")
        results = vectorstore.similarity_search(query, k=3)
        
        for i, doc in enumerate(results, 1):
            pillar = doc.metadata.get('pillar', 'Unknown')
            page = doc.metadata.get('page', '?')
            bp_id = doc.metadata.get('best_practice_id', '-')
            
            print(f"\n  [{i}] {pillar} | Page {page} | BP: {bp_id}")
            print(f"      {doc.page_content[:200]}...")


if __name__ == "__main__":
    # 1. ë¬¸ì„œ ë¡œë”© + ì •ì œ
    docs = load_and_clean_documents(DOCUMENT_DIR)
    
    # 2. Chunking
    chunks = split_documents(docs)
    
    # 3. Pillar ë¶„í¬ í™•ì¸
    pillar_counts = {}
    bp_counts = 0
    for chunk in chunks:
        pillar = chunk.metadata.get("pillar", "Unknown")
        pillar_counts[pillar] = pillar_counts.get(pillar, 0) + 1
        if chunk.metadata.get("best_practice_id"):
            bp_counts += 1
    
    print("\n" + "="*50)
    print("ğŸ“Š Pillar ë¶„í¬")
    print("="*50)
    for pillar, count in sorted(pillar_counts.items(), key=lambda x: -x[1]):
        print(f"  {pillar}: {count}ê°œ")
    
    print(f"\nğŸ“Œ BP IDê°€ ìˆëŠ” ì²­í¬: {bp_counts}ê°œ ({bp_counts*100//len(chunks)}%)")
    
    # 4. Embedding + FAISS ì €ì¥
    vectorstore = create_vectorstore(chunks)
    
    # 5. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    test_retrieval(vectorstore)
    
    print("\n" + "="*50)
    print("âœ… ì™„ë£Œ!")
    print("="*50)
    print(f"ë‹¤ìŒ ë‹¨ê³„: AWS ì½˜ì†”ì—ì„œ {VECTOR_DB_DIR}/ í´ë”ì˜")
    print("index.faissì™€ index.pklì„ S3ì— ì—…ë¡œë“œí•˜ì„¸ìš”.")
    print(f"S3 ê²½ë¡œ: s3://korea-sw-16-chatbot-s3/vectorstore/")

# if __name__ == "__main__":
#     docs = load_and_clean_documents(DOCUMENT_DIR)
#     chunks = split_documents(docs)
    
#     # Pillar ë¶„í¬ í™•ì¸
#     pillar_counts = {}
#     bp_counts = 0
#     for chunk in chunks:
#         pillar = chunk.metadata.get("pillar", "Unknown")
#         pillar_counts[pillar] = pillar_counts.get(pillar, 0) + 1
#         if chunk.metadata.get("best_practice_id"):
#             bp_counts += 1
    
#     print("\n" + "="*50)
#     print("ğŸ“Š Pillar ë¶„í¬")
#     print("="*50)
#     for pillar, count in sorted(pillar_counts.items(), key=lambda x: -x[1]):
#         print(f"  {pillar}: {count}ê°œ")
    
#     print(f"\nğŸ“Œ BP IDê°€ ìˆëŠ” ì²­í¬: {bp_counts}ê°œ ({bp_counts*100//len(chunks)}%)")
    
#     # ìƒ˜í”Œ í™•ì¸
#     print("\n" + "="*50)
#     print("ìƒ˜í”Œ ì²­í¬ í™•ì¸")
#     print("="*50)
    
#     for i in [0, 100, 500, 1000, 1500]:
#         if i < len(chunks):
#             c = chunks[i]
#             print(f"\n--- ì²­í¬ #{i} ---")
#             print(f"Pillar: {c.metadata.get('pillar')}")
#             print(f"BP ID: {c.metadata.get('best_practice_id') or '(ì—†ìŒ)'}")
#             print(f"Page: {c.metadata.get('page')}")
#             print(f"Content: {c.page_content[:200]}...")
        