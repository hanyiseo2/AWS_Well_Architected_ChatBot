import os
import boto3
from langchain_aws import BedrockEmbeddings, ChatBedrock
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# ===== ì„¤ì • =====
AWS_REGION = "us-east-1"
S3_BUCKET = "your-bucket-name"
S3_PREFIX = "vectorstore/"
USE_S3 = False  # True: S3ì—ì„œ ë¡œë“œ, False: ë¡œì»¬

print("ğŸ¤– AWS RAG ì±—ë´‡ ì´ˆê¸°í™”\n")

# ===== 1. S3ì—ì„œ ë²¡í„° ìŠ¤í† ì–´ ë‹¤ìš´ë¡œë“œ (ì˜µì…˜) =====
if USE_S3:
    print("â˜ï¸  S3ì—ì„œ ë²¡í„° ìŠ¤í† ì–´ ë‹¤ìš´ë¡œë“œ ì¤‘...")
    s3_client = boto3.client('s3', region_name=AWS_REGION)
    
    os.makedirs("vectorstore", exist_ok=True)
    
    files = ['index.faiss', 'index.pkl']
    for file_name in files:
        s3_key = f"{S3_PREFIX}{file_name}"
        local_path = f"vectorstore/{file_name}"
        
        s3_client.download_file(S3_BUCKET, s3_key, local_path)
        print(f"   âœ… {file_name}")
    
    print("   ì™„ë£Œ\n")

# ===== 2. Embeddings ì´ˆê¸°í™” =====
print("ğŸ”¢ Embeddings ì´ˆê¸°í™”...")
try:
    embeddings = BedrockEmbeddings(
        model_id="amazon.titan-embed-text-v2:0",
        region_name=AWS_REGION
    )
    print("   âœ… Bedrock Titan Embeddings\n")
except Exception as e:
    print(f"   âš ï¸ Bedrock ì‹¤íŒ¨, OpenAIë¡œ í´ë°±\n")
    from langchain_openai import OpenAIEmbeddings
    embeddings = OpenAIEmbeddings()

# ===== 3. FAISS ë¡œë“œ (Dense Vector) =====
print("ğŸ“Š FAISS ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ...")
vectorstore = FAISS.load_local(
    "vectorstore", 
    embeddings, 
    allow_dangerous_deserialization=True
)
faiss_retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}
)
print("   âœ… Dense Vector Retriever\n")

# ===== 4. BM25 Retriever (Sparse Vector) =====
print("ğŸ“ BM25 Retriever êµ¬ì„±...")

# ì›ë³¸ ì²­í¬ ì¬êµ¬ì„± (BM25ìš©)
local_pdfs = [
    "./docs/wellarchitected-machine-learning-lens.pdf",
]

all_docs = []
for pdf in local_pdfs:
    if os.path.exists(pdf):
        loader = PyPDFLoader(pdf)
        all_docs.extend(loader.load())

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000, 
    chunk_overlap=200
)
chunks = text_splitter.split_documents(all_docs)

bm25_retriever = BM25Retriever.from_documents(chunks)
bm25_retriever.k = 5
print(f"   âœ… Sparse Vector Retriever ({len(chunks)}ê°œ ì²­í¬)\n")

# ===== 5. Hybrid Retriever (RRF) =====
print("ğŸ”— Hybrid Search êµ¬ì„±...")
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, faiss_retriever],
    weights=[0.5, 0.5]  # ë™ì¼ ê°€ì¤‘ì¹˜
)
print("   âœ… RRF (Reciprocal Rank Fusion)\n")

# ===== 6. LLM ì´ˆê¸°í™” =====
print("ğŸ§  LLM ì´ˆê¸°í™”...")
try:
    llm = ChatBedrock(
        model_id="anthropic.claude-3-5-sonnet-20240620-v1:0",
        region_name=AWS_REGION,
        model_kwargs={"temperature": 0.1}
    )
    print("   âœ… Bedrock Claude 3.5 Sonnet\n")
except Exception as e:
    print(f"   âš ï¸ Bedrock ì‹¤íŒ¨, OpenAIë¡œ í´ë°±\n")
    from langchain_openai import ChatOpenAI
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.1)

# ===== 7. RAG Chain =====
print("âš™ï¸  RAG Chain êµ¬ì„±...\n")

template = """You are an expert on AWS Well-Architected Framework.

**Instructions:**
1. Answer based ONLY on the provided context
2. Structure answers with bullet points
3. Include specific AWS services mentioned
4. Cite sources at the end

Context:
{context}

Question: {question}

Answer:"""

PROMPT = PromptTemplate(template=template, input_variables=["context", "question"])

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=ensemble_retriever,  # Hybrid!
    return_source_documents=True,
    chain_type_kwargs={"prompt": PROMPT}
)

print("âœ… ì´ˆê¸°í™” ì™„ë£Œ!\n")

# ===== 8. ì§ˆë¬¸ í•¨ìˆ˜ =====
def ask_question(query):
    """ì§ˆë¬¸-ë‹µë³€ í•¨ìˆ˜"""
    print(f"\n{'='*70}")
    print(f"â“ ì§ˆë¬¸: {query}")
    print(f"{'='*70}\n")
    print("ğŸ” Hybrid Search ìˆ˜í–‰ ì¤‘...\n")
    
    result = qa_chain.invoke({"query": query})
    answer = result['result']
    source_docs = result['source_documents']
    
    print(f"ğŸ’¡ ë‹µë³€:\n{answer}\n")
    
    # ì¶œì²˜ ì •ë¦¬
    print(f"ğŸ“š ì°¸ì¡° ì¶œì²˜ ({len(source_docs)}ê°œ):")
    sources = {}
    for doc in source_docs:
        source = doc.metadata.get('source', 'Unknown')
        doc_name = doc.metadata.get('doc_name', source.split('/')[-1])
        page = doc.metadata.get('page', 'N/A')
        
        if doc_name not in sources:
            sources[doc_name] = []
        sources[doc_name].append(page)
    
    for idx, (doc_name, pages) in enumerate(sources.items(), 1):
        pages_str = ', '.join(map(str, sorted(set(pages))[:3]))
        print(f"  [{idx}] {doc_name} (í˜ì´ì§€: {pages_str})")
    
    return answer

# ===== Main =====
if __name__ == "__main__":
    print("="*70)
    print("ğŸ¤– AWS Well-Architected Chatbot")
    print("   - Hybrid Search (BM25 + FAISS)")
    print("   - Bedrock Claude 3.5 + Titan Embeddings")
    print("="*70)
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    test_questions = [
        "What are security best practices for ML models?",
        "How to optimize costs in generative AI?",
    ]
    
    print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ:\n")
    for q in test_questions:
        ask_question(q)
        print("\n")
    
    # ëŒ€í™”í˜• ëª¨ë“œ
    print("="*70)
    print("ğŸ’¬ ëŒ€í™” ëª¨ë“œ (ì¢…ë£Œ: 'quit')")
    print("="*70)
    
    while True:
        query = input("\nğŸ§‘ ì§ˆë¬¸: ").strip()
        if query.lower() in ['quit', 'exit', 'q']:
            print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        if query:
            ask_question(query)